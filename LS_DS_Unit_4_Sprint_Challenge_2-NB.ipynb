{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2*\n",
    "\n",
    "# Sprint Challenge - Neural Network Foundations\n",
    "\n",
    "Table of Problems\n",
    "\n",
    "1. [Defining Neural Networks](#Q1)\n",
    "2. [Chocolate Gummy Bears](#Q2)\n",
    "    - Perceptron\n",
    "    - Multilayer Perceptron\n",
    "4. [Keras MMP](#Q3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Q1\"></a>\n",
    "## 1. Define the following terms:\n",
    "\n",
    "- **Neuron:**\n",
    " neuron is a mathematical function, typically, it computes the weighted average of its input, and this sum is passed through an activation function to generate an output\n",
    "- **Input Layer:**\n",
    "It's the initial data for the neural network\n",
    "- **Hidden Layer:**\n",
    "A hidden layer is a layer in between input layers and output layers, where neurons take in a set of weighted inputs and produce an output through an activation function.\n",
    "- **Output Layer:**\n",
    "It is the final neuron/neurons which produces the result for given inputs\n",
    "- **Activation:**\n",
    "The activation function defines the strength of the output of that node given an input or set of inputs. Examples include Sigmoid, Relu, Softmax etc.\n",
    "- **Backpropagation:**\n",
    "It is a widely used algorithm in training feedforward neural networks. It  computes the gradient of the loss function with respect to the weights of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chocolate Gummy Bears <a id=\"Q2\"></a>\n",
    "\n",
    "Right now, you're probably thinking, \"yuck, who the hell would eat that?\". Great question. Your candy company wants to know too. And you thought I was kidding about the [Chocolate Gummy Bears](https://nuts.com/chocolatessweets/gummies/gummy-bears/milk-gummy-bears.html?utm_source=google&utm_medium=cpc&adpos=1o1&gclid=Cj0KCQjwrfvsBRD7ARIsAKuDvMOZrysDku3jGuWaDqf9TrV3x5JLXt1eqnVhN0KM6fMcbA1nod3h8AwaAvWwEALw_wcB). \n",
    "\n",
    "Let's assume that a candy company has gone out and collected information on the types of Halloween candy kids ate. Our candy company wants to predict the eating behavior of witches, warlocks, and ghosts -- aka costumed kids. They shared a sample dataset with us. Each row represents a piece of candy that a costumed child was presented with during \"trick\" or \"treat\". We know if the candy was `chocolate` (or not chocolate) or `gummy` (or not gummy). Your goal is to predict if the costumed kid `ate` the piece of candy. \n",
    "\n",
    "If both chocolate and gummy equal one, you've got a chocolate gummy bear on your hands!?!?!\n",
    "![Chocolate Gummy Bear](https://ed910ae2d60f0d25bcb8-80550f96b5feb12604f4f720bfefb46d.ssl.cf1.rackcdn.com/3fb630c04435b7b5-2leZuM7_-zoom.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "candy = pd.read_csv('chocolate_gummy_bears.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chocolate</th>\n",
       "      <th>gummy</th>\n",
       "      <th>ate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chocolate  gummy  ate\n",
       "0          0      1    1\n",
       "1          1      0    1\n",
       "2          0      1    1\n",
       "3          0      0    0\n",
       "4          1      1    0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron\n",
    "\n",
    "To make predictions on the `candy` dataframe. Build and train a Perceptron using numpy. Your target column is `ate` and your features: `chocolate` and `gummy`. Do not do any feature engineering. :P\n",
    "\n",
    "Once you've trained your model, report your accuracy. You will not be able to achieve more than ~50% with the simple perceptron. Explain why you could not achieve a higher accuracy with the *simple perceptron* architecture, because it's possible to achieve ~95% accuracy on this dataset. Provide your answer in markdown (and *optional* data anlysis code) after your perceptron implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# Start your candy perceptron here\n",
    "\n",
    "X = candy[['chocolate', 'gummy']].values\n",
    "y = candy['ate'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAOuklEQVR4nO3df6jdd33H8ecrzaJsq3GYK0iSmspSMJRB3aXrEGal3UjLSP7pXAKijmLQre4PZdDRkUn8Z+vYxEE2DU78AVqrf+hFIh3TikOMy+2q1aRk3MVqLpX1ql3+kdqGvvfHOa1nJ+fe873pOefmfnw+4MD5fs+n57y/5948c3J+9KSqkCRtfls2egBJ0mQYdElqhEGXpEYYdElqhEGXpEZs3agb3rFjR+3Zs2ejbl6SNqVHHnnkx1U1N+qyDQv6nj17WFxc3Kibl6RNKckPVrvMp1wkqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaMfaDRUk+Bvwh8FRV3Tji8gAfAu4Efga8o6r+c9KDDnv+6HaSX2xXwZZjF6d9s5K0LrNsVZdH6B8H9q9x+R3A3v7pCPDPL32stb1wBw2fnj+6fdo3LUmdzbpVY4NeVV8HfrrGkoPAJ6vnFPDKJK+Z1ICjvHCnjNsnSRtp1q2axHPoO4ELA9vL/X2XSXIkyWKSxZWVlQnctCTpBZMI+qi/a0Z+UWlVnaiq+aqan5sb+T8LkyRdoUkEfRnYPbC9C3hyAte7qqreadw+SdpIs27VJIK+ALwtPbcAF6vqRxO43lVtOXbxxTtl8OS7XCRdTWbdqi5vW/wMcCuwI8ky8NfArwBU1YeBk/TesrhE722LfzKVSYcM3yG+HirpajTLVo0NelUdHnN5AX82sYkkSVfET4pKUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiM6BT3J/iTnkiwluXfE5dcleTjJo0keS3Ln5EeVJK1lbNCTXAMcB+4A9gGHk+wbWvZXwINVdRNwCPinSQ8qSVpbl0foNwNLVXW+qp4FHgAODq0p4BX989uBJyc3oiSpi60d1uwELgxsLwO/M7Tm/cC/JnkP8GvA7ROZTpLUWZdH6Bmxr4a2DwMfr6pdwJ3Ap5Jcdt1JjiRZTLK4srKy/mklSavqEvRlYPfA9i4uf0rlbuBBgKr6JvByYMfwFVXViaqar6r5ubm5K5tYkjRSl6CfBvYmuT7JNnovei4MrfkhcBtAktfTC7oPwSVphsYGvaouAfcADwGP03s3y5kkx5Ic6C97H/DOJN8BPgO8o6qGn5aRJE1RlxdFqaqTwMmhfUcHzp8F3jjZ0SRJ6+EnRSWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhrRKehJ9ic5l2Qpyb2rrHlLkrNJziT59GTHlCSNs3XcgiTXAMeB3weWgdNJFqrq7MCavcBfAm+sqqeTvHpaA0uSRuvyCP1mYKmqzlfVs8ADwMGhNe8EjlfV0wBV9dRkx5QkjdMl6DuBCwPby/19g24AbkjyjSSnkuwfdUVJjiRZTLK4srJyZRNLkkbqEvSM2FdD21uBvcCtwGHgo0leedl/VHWiquaran5ubm69s0qS1tAl6MvA7oHtXcCTI9Z8saqeq6rvA+foBV6SNCNdgn4a2Jvk+iTbgEPAwtCaLwBvBkiyg95TMOcnOagkaW1jg15Vl4B7gIeAx4EHq+pMkmNJDvSXPQT8JMlZ4GHgL6rqJ9MaWpJ0uVQNPx0+G/Pz87W4uLghty1Jm1WSR6pqftRlflJUkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhrRKehJ9ic5l2Qpyb1rrLsrSSWZn9yIkqQuxgY9yTXAceAOYB9wOMm+EeuuBf4c+Nakh5QkjdflEfrNwFJVna+qZ4EHgIMj1n0AuB94ZoLzSZI66hL0ncCFge3l/r4XJbkJ2F1VX1rripIcSbKYZHFlZWXdw0qSVtcl6Bmxr168MNkCfBB437grqqoTVTVfVfNzc3Pdp5QkjdUl6MvA7oHtXcCTA9vXAjcCX0vyBHALsOALo5I0W12CfhrYm+T6JNuAQ8DCCxdW1cWq2lFVe6pqD3AKOFBVi1OZWJI00tigV9Ul4B7gIeBx4MGqOpPkWJID0x5QktTN1i6LquokcHJo39FV1t760seSJK2XnxSVpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEZ0CnqS/UnOJVlKcu+Iy9+b5GySx5J8JclrJz+qJGktY4Oe5BrgOHAHsA84nGTf0LJHgfmq+i3g88D9kx5UkrS2Lo/QbwaWqup8VT0LPAAcHFxQVQ9X1c/6m6eAXZMdU5I0Tpeg7wQuDGwv9/et5m7gyy9lKEnS+m3tsCYj9tXIhclbgXngTatcfgQ4AnDdddd1HFGS1EWXR+jLwO6B7V3Ak8OLktwO3AccqKqfj7qiqjpRVfNVNT83N3cl80qSVtEl6KeBvUmuT7INOAQsDC5IchPwEXoxf2ryY0qSxhkb9Kq6BNwDPAQ8DjxYVWeSHEtyoL/s74BfBz6X5NtJFla5OknSlHR5Dp2qOgmcHNp3dOD87ROeS5K0Tn5SVJIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqRFbuyxKsh/4EHAN8NGq+puhy18GfBL4beAnwB9X1ROTHfX/e/7odpJfbFfBlmMXp3mTkrRus2zV2EfoSa4BjgN3APuAw0n2DS27G3i6qn4T+CDwt5MedNALd9Dw6fmj26d5s5K0LrNuVZenXG4GlqrqfFU9CzwAHBxacxD4RP/854HbksG/kybrhTtl3D5J2kizblWXoO8ELgxsL/f3jVxTVZeAi8Crhq8oyZEki0kWV1ZWrmxiSdJIXYI+6u+SuoI1VNWJqpqvqvm5ubku80mSOuoS9GVg98D2LuDJ1dYk2QpsB346iQFHqeqdxu2TpI0061Z1CfppYG+S65NsAw4BC0NrFoC398/fBXy1anp53XLs4ot3yuDJd7lIuprMulVj37ZYVZeS3AM8RO9tix+rqjNJjgGLVbUA/AvwqSRL9B6ZH5rKtAOG7xBfD5V0NZplqzq9D72qTgInh/YdHTj/DPBHkx1NkrQeflJUkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEQZdkhqRKX6gc+0bTlaAH0zgqnYAP57A9WwWHm+7fpmOFTzeK/Xaqhr5P8PasKBPSpLFqprf6DlmxeNt1y/TsYLHOw0+5SJJjTDoktSIFoJ+YqMHmDGPt12/TMcKHu/Ebfrn0CVJPS08QpckYdAlqRmbJuhJ9ic5l2Qpyb0jLn9Zks/2L/9Wkj2zn3JyOhzve5OcTfJYkq8kee1GzDkJ4451YN1dSSrJpn6rW5fjTfKW/s/3TJJPz3rGSerwu3xdkoeTPNr/fb5zI+achCQfS/JUku+tcnmS/GP/vngsyRsmOkBVXfUnet+U9N/A64BtwHeAfUNr/hT4cP/8IeCzGz33lI/3zcCv9s+/e7Meb5dj7a+7Fvg6cAqY3+i5p/yz3Qs8CvxGf/vVGz33lI/3BPDu/vl9wBMbPfdLON7fA94AfG+Vy+8Evkzvi4tuAb41ydvfLI/QbwaWqup8VT0LPAAcHFpzEPhE//zngduSbNZvpht7vFX1cFX9rL95it6Xd29GXX62AB8A7geemeVwU9DleN8JHK+qpwGq6qkZzzhJXY63gFf0z2/n8i+h3zSq6uv0voZzNQeBT1bPKeCVSV4zqdvfLEHfCVwY2F7u7xu5pqouAReBV81kusnrcryD7qb3t/5mNPZYk9wE7K6qL81ysCnp8rO9AbghyTeSnEqyf2bTTV6X430/8NYky/S+6vI9sxltQ6z3z/a6dPpO0avAqEfaw++37LJms+h8LEneCswDb5rqRNOz5rEm2QJ8EHjHrAaasi4/2630nna5ld6/vP49yY1V9b9Tnm0auhzvYeDjVfX3SX6X3hfO31hVz09/vJmbaqc2yyP0ZWD3wPYuLv9n2Ytrkmyl90+3tf7pczXrcrwkuR24DzhQVT+f0WyTNu5YrwVuBL6W5Al6zzsubOIXRrv+Ln+xqp6rqu8D5+gFfjPqcrx3Aw8CVNU3gZfT+x9ZtajTn+0rtVmCfhrYm+T6JNvovei5MLRmAXh7//xdwFer/yrEJjT2ePtPQ3yEXsw383Osax5rVV2sqh1Vtaeq9tB7veBAVS1uzLgvWZff5S/Qe9GbJDvoPQVzfqZTTk6X4/0hcBtAktfTC/rKTKecnQXgbf13u9wCXKyqH03s2jf6VeF1vHp8J/Bf9F4xv6+/7xi9P9zQ+yX4HLAE/Afwuo2eecrH+2/A/wDf7p8WNnrmaR3r0NqvsYnf5dLxZxvgH4CzwHeBQxs985SPdx/wDXrvgPk28AcbPfNLONbPAD8CnqP3aPxu4F3AuwZ+tsf798V3J/277Ef/JakRm+UpF0nSGAZdkhph0CWpEQZdkhph0CWpEQZdkhph0CWpEf8HCY5UELgbvIgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X[:, 0], y)\n",
    "plt.scatter(X[:, 1], y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The accuracy of this single perceptron is going to low because the dataset is not Linear and Perceptrons are not good for that kind of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(object):\n",
    "  def __init__(self, rate = 0.01, niter = 10):\n",
    "    self.rate = rate\n",
    "    self.niter = niter\n",
    "\n",
    "  def fit(self, X, y):\n",
    "    \"\"\"Fit training data\n",
    "    X : Training vectors, X.shape : [#samples, #features]\n",
    "    y : Target values, y.shape : [#samples]\n",
    "    \"\"\"\n",
    "\n",
    "    # weights\n",
    "    self.weight = np.zeros(1 + X.shape[1])\n",
    "\n",
    "    # Number of misclassifications\n",
    "    self.errors = []  # Number of misclassifications\n",
    "\n",
    "    for i in range(self.niter):\n",
    "      err = 0\n",
    "      for xi, target in zip(X, y):\n",
    "        delta_w = self.rate * (target - self.predict(xi))\n",
    "        self.weight[1:] += delta_w * xi\n",
    "        self.weight[0] += delta_w\n",
    "        err += int(delta_w != 0.0)\n",
    "      self.errors.append(err)\n",
    "    return self\n",
    "\n",
    "  def net_input(self, X):\n",
    "    \"\"\"Calculate net input\"\"\"\n",
    "    return np.dot(X, self.weight[1:]) + self.weight[0]\n",
    "\n",
    "  def predict(self, X):\n",
    "    \"\"\"Return class label after unit step\"\"\"\n",
    "    return np.where(self.net_input(X) >= 0.0, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Perceptron at 0x1be0ca7b748>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pn = Perceptron(0.1, 100)\n",
    "pn.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_x= len(X)\n",
    "predictions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len_x):\n",
    "    predictions.append(pn.predict(X[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: \n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss: \\n\" + str(np.mean(np.square(y - pn.predict(X))))) # mean sum squared loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference: [0 0 0 ... 0 0 0]\n",
      "Difference Squared: [0 0 0 ... 0 0 0]\n",
      "Mean Squared Error 0.5\n"
     ]
    }
   ],
   "source": [
    "difference = y-predictions\n",
    "print(\"Difference:\", difference)\n",
    "difference_squared = difference**2\n",
    "print(\"Difference Squared:\", difference_squared)\n",
    "MSE = difference_squared.sum()/len(difference_squared)\n",
    "print(\"Mean Squared Error\", MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron <a id=\"Q3\"></a>\n",
    "\n",
    "Using the sample candy dataset, implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights. Your Multilayer Perceptron should be implemented in Numpy. \n",
    "Your network must have one hidden layer.\n",
    "\n",
    "Once you've trained your model, report your accuracy. Explain why your MLP's performance is considerably better than your simple perceptron's on the candy dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork: \n",
    "    def __init__(self):\n",
    "        # Set upArchietecture \n",
    "        self.inputs = 2\n",
    "        self.hiddenNodes = 25\n",
    "        self.outputNodes = 1\n",
    "        #Initial weights\n",
    "        self.weights1 = np.random.randn(self.inputs, self.hiddenNodes) #2x3\n",
    "        self.weights2 = np.random.rand(self.hiddenNodes, self.outputNodes) #3x1\n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1+np.exp(-s))\n",
    "    def sigmoidPrime(self, s):\n",
    "        return s * (1 - s)\n",
    "    def feed_forward(self, X):\n",
    "        \"\"\"\n",
    "        Calculate the NN inference using feed forward.\n",
    "        \"\"\"\n",
    "        #Weighted sume of inputs and hidden layer\n",
    "        self.hidden_sum = np.dot(X, self.weights1)\n",
    "        #Acivations of weighted sum\n",
    "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
    "        # Weight sum between hidden and output\n",
    "        self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
    "        #Final activation of output\n",
    "        self.activated_output = self.sigmoid(self.output_sum)\n",
    "        return self.activated_output\n",
    "    def backward(self, X, y, o):\n",
    "        \"\"\"\n",
    "        Backward propagate through the network\n",
    "        \"\"\"\n",
    "        self.o_error = y - o[:,0] #error in output\n",
    "        self.o_delta = self.o_error * self.sigmoidPrime(o[:,0]) # apply derivative of sigmoid to error\n",
    "        self.o_delta = np.reshape(self.o_delta,[self.o_delta.shape[0], 1])\n",
    "        self.z2_error = self.o_delta.dot(self.weights2.T) # z2 error: how much our hidden layer weights were off\n",
    "        self.z2_delta = self.z2_error*self.sigmoidPrime(self.activated_hidden)\n",
    "        self.weights1 += X.T.dot(self.z2_delta) #Adjust first set (input => hidden) weights\n",
    "        self.weights2 += self.activated_hidden.T.dot(self.o_delta) #adjust second set (hidden => output) weights\n",
    "    def train(self, X, y):\n",
    "        o = self.feed_forward(X)\n",
    "        self.backward(X, y, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------EPOCH 1---------+\n",
      "Input: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "Actual Output: \n",
      " [1 1 1 ... 1 1 1]\n",
      "Predicted Output: \n",
      " [[0.99891047]\n",
      " [0.99681101]\n",
      " [0.99891047]\n",
      " ...\n",
      " [0.99891047]\n",
      " [0.99891047]\n",
      " [0.99681101]]\n",
      "Loss: \n",
      " 0.4979400099327439\n",
      "+---------EPOCH 2---------+\n",
      "Input: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "Actual Output: \n",
      " [1 1 1 ... 1 1 1]\n",
      "Predicted Output: \n",
      " [[1.19055409e-24]\n",
      " [2.82254091e-21]\n",
      " [1.19055409e-24]\n",
      " ...\n",
      " [1.19055409e-24]\n",
      " [1.19055409e-24]\n",
      " [2.82254091e-21]]\n",
      "Loss: \n",
      " 0.5\n",
      "+---------EPOCH 3---------+\n",
      "Input: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "Actual Output: \n",
      " [1 1 1 ... 1 1 1]\n",
      "Predicted Output: \n",
      " [[1.19055409e-24]\n",
      " [2.82254091e-21]\n",
      " [1.19055409e-24]\n",
      " ...\n",
      " [1.19055409e-24]\n",
      " [1.19055409e-24]\n",
      " [2.82254091e-21]]\n",
      "Loss: \n",
      " 0.5\n",
      "+---------EPOCH 4---------+\n",
      "Input: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "Actual Output: \n",
      " [1 1 1 ... 1 1 1]\n",
      "Predicted Output: \n",
      " [[1.19055409e-24]\n",
      " [2.82254091e-21]\n",
      " [1.19055409e-24]\n",
      " ...\n",
      " [1.19055409e-24]\n",
      " [1.19055409e-24]\n",
      " [2.82254091e-21]]\n",
      "Loss: \n",
      " 0.5\n",
      "+---------EPOCH 5---------+\n",
      "Input: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "Actual Output: \n",
      " [1 1 1 ... 1 1 1]\n",
      "Predicted Output: \n",
      " [[1.19055409e-24]\n",
      " [2.82254091e-21]\n",
      " [1.19055409e-24]\n",
      " ...\n",
      " [1.19055409e-24]\n",
      " [1.19055409e-24]\n",
      " [2.82254091e-21]]\n",
      "Loss: \n",
      " 0.5\n",
      "+---------EPOCH 1000---------+\n",
      "Input: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "Actual Output: \n",
      " [1 1 1 ... 1 1 1]\n",
      "Predicted Output: \n",
      " [[1.19055409e-24]\n",
      " [2.82254091e-21]\n",
      " [1.19055409e-24]\n",
      " ...\n",
      " [1.19055409e-24]\n",
      " [1.19055409e-24]\n",
      " [2.82254091e-21]]\n",
      "Loss: \n",
      " 0.5\n",
      "+---------EPOCH 2000---------+\n",
      "Input: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "Actual Output: \n",
      " [1 1 1 ... 1 1 1]\n",
      "Predicted Output: \n",
      " [[1.19055409e-24]\n",
      " [2.82254091e-21]\n",
      " [1.19055409e-24]\n",
      " ...\n",
      " [1.19055409e-24]\n",
      " [1.19055409e-24]\n",
      " [2.82254091e-21]]\n",
      "Loss: \n",
      " 0.5\n",
      "+---------EPOCH 3000---------+\n",
      "Input: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "Actual Output: \n",
      " [1 1 1 ... 1 1 1]\n",
      "Predicted Output: \n",
      " [[1.19055409e-24]\n",
      " [2.82254091e-21]\n",
      " [1.19055409e-24]\n",
      " ...\n",
      " [1.19055409e-24]\n",
      " [1.19055409e-24]\n",
      " [2.82254091e-21]]\n",
      "Loss: \n",
      " 0.5\n",
      "+---------EPOCH 4000---------+\n",
      "Input: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "Actual Output: \n",
      " [1 1 1 ... 1 1 1]\n",
      "Predicted Output: \n",
      " [[1.19055409e-24]\n",
      " [2.82254091e-21]\n",
      " [1.19055409e-24]\n",
      " ...\n",
      " [1.19055409e-24]\n",
      " [1.19055409e-24]\n",
      " [2.82254091e-21]]\n",
      "Loss: \n",
      " 0.5\n",
      "+---------EPOCH 5000---------+\n",
      "Input: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "Actual Output: \n",
      " [1 1 1 ... 1 1 1]\n",
      "Predicted Output: \n",
      " [[1.19055409e-24]\n",
      " [2.82254091e-21]\n",
      " [1.19055409e-24]\n",
      " ...\n",
      " [1.19055409e-24]\n",
      " [1.19055409e-24]\n",
      " [2.82254091e-21]]\n",
      "Loss: \n",
      " 0.5\n",
      "+---------EPOCH 6000---------+\n",
      "Input: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "Actual Output: \n",
      " [1 1 1 ... 1 1 1]\n",
      "Predicted Output: \n",
      " [[1.19055409e-24]\n",
      " [2.82254091e-21]\n",
      " [1.19055409e-24]\n",
      " ...\n",
      " [1.19055409e-24]\n",
      " [1.19055409e-24]\n",
      " [2.82254091e-21]]\n",
      "Loss: \n",
      " 0.5\n",
      "+---------EPOCH 7000---------+\n",
      "Input: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "Actual Output: \n",
      " [1 1 1 ... 1 1 1]\n",
      "Predicted Output: \n",
      " [[1.19055409e-24]\n",
      " [2.82254091e-21]\n",
      " [1.19055409e-24]\n",
      " ...\n",
      " [1.19055409e-24]\n",
      " [1.19055409e-24]\n",
      " [2.82254091e-21]]\n",
      "Loss: \n",
      " 0.5\n",
      "+---------EPOCH 8000---------+\n",
      "Input: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "Actual Output: \n",
      " [1 1 1 ... 1 1 1]\n",
      "Predicted Output: \n",
      " [[1.19055409e-24]\n",
      " [2.82254091e-21]\n",
      " [1.19055409e-24]\n",
      " ...\n",
      " [1.19055409e-24]\n",
      " [1.19055409e-24]\n",
      " [2.82254091e-21]]\n",
      "Loss: \n",
      " 0.5\n",
      "+---------EPOCH 9000---------+\n",
      "Input: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "Actual Output: \n",
      " [1 1 1 ... 1 1 1]\n",
      "Predicted Output: \n",
      " [[1.19055409e-24]\n",
      " [2.82254091e-21]\n",
      " [1.19055409e-24]\n",
      " ...\n",
      " [1.19055409e-24]\n",
      " [1.19055409e-24]\n",
      " [2.82254091e-21]]\n",
      "Loss: \n",
      " 0.5\n",
      "+---------EPOCH 10000---------+\n",
      "Input: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "Actual Output: \n",
      " [1 1 1 ... 1 1 1]\n",
      "Predicted Output: \n",
      " [[1.19055409e-24]\n",
      " [2.82254091e-21]\n",
      " [1.19055409e-24]\n",
      " ...\n",
      " [1.19055409e-24]\n",
      " [1.19055409e-24]\n",
      " [2.82254091e-21]]\n",
      "Loss: \n",
      " 0.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "nn = NeuralNetwork()\n",
    "\n",
    "# Number of Epochs / Iterations\n",
    "for i in range(10000):\n",
    "    if (i+1 in [1,2,3,4,5]) or ((i+1) % 1000 ==0):\n",
    "        print('+' + '---' * 3 + f'EPOCH {i+1}' + '---'*3 + '+')\n",
    "        print('Input: \\n', X)\n",
    "        print('Actual Output: \\n', y)\n",
    "        print('Predicted Output: \\n', str(nn.feed_forward(X)))\n",
    "        print(\"Loss: \\n\", str(np.mean(np.square(y - nn.feed_forward(X)))))\n",
    "    nn.train(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: \n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss: \\n\" + str(np.mean(np.square(y - nn.feed_forward(X))))) # mean sum squared loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loss is supposed to be better since we are correcting the weights constantly, using back propogation algorithm to yield better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Keras MMP <a id=\"Q3\"></a>\n",
    "\n",
    "Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy.\n",
    "Use the Heart Disease Dataset (binary classification)\n",
    "Use an appropriate loss function for a binary classification task\n",
    "Use an appropriate activation function on the final layer of your network.\n",
    "Train your model using verbose output for ease of grading.\n",
    "Use GridSearchCV or RandomSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
    "When hyperparameter tuning, show you work by adding code cells for each new experiment.\n",
    "Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
    "You must hyperparameter tune at least 3 parameters in order to get a 3 on this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>120</td>\n",
       "      <td>240</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>194</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>122</td>\n",
       "      <td>222</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>186</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "      <td>149</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>125</td>\n",
       "      <td>0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>122</td>\n",
       "      <td>213</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>165</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "103   42    1   2       120   240    1        1      194      0      0.8   \n",
       "56    48    1   0       122   222    0        0      186      0      0.0   \n",
       "151   71    0   0       112   149    0        1      125      0      1.6   \n",
       "302   57    0   1       130   236    0        0      174      0      0.0   \n",
       "74    43    0   2       122   213    0        1      165      0      0.2   \n",
       "\n",
       "     slope  ca  thal  target  \n",
       "103      0   0     3       1  \n",
       "56       2   0     2       1  \n",
       "151      1   0     2       1  \n",
       "302      1   1     2       0  \n",
       "74       1   0     2       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv')\n",
    "df = df.sample(frac=1)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.values[:,0:13]\n",
    "y = df.values[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "303/303 [==============================] - 0s 356us/sample - loss: 2.9628 - mean_squared_error: 2.9628 - acc: 0.4554\n",
      "Epoch 2/10\n",
      "303/303 [==============================] - 0s 43us/sample - loss: 0.9995 - mean_squared_error: 0.9995 - acc: 0.4488\n",
      "Epoch 3/10\n",
      "303/303 [==============================] - 0s 49us/sample - loss: 0.7292 - mean_squared_error: 0.7292 - acc: 0.4455\n",
      "Epoch 4/10\n",
      "303/303 [==============================] - 0s 46us/sample - loss: 0.6012 - mean_squared_error: 0.6012 - acc: 0.4422\n",
      "Epoch 5/10\n",
      "303/303 [==============================] - 0s 43us/sample - loss: 0.5393 - mean_squared_error: 0.5393 - acc: 0.4554\n",
      "Epoch 6/10\n",
      "303/303 [==============================] - 0s 43us/sample - loss: 0.5179 - mean_squared_error: 0.5179 - acc: 0.4620\n",
      "Epoch 7/10\n",
      "303/303 [==============================] - 0s 109us/sample - loss: 0.5001 - mean_squared_error: 0.5001 - acc: 0.4587\n",
      "Epoch 8/10\n",
      "303/303 [==============================] - 0s 43us/sample - loss: 0.4866 - mean_squared_error: 0.4866 - acc: 0.4620\n",
      "Epoch 9/10\n",
      "303/303 [==============================] - 0s 46us/sample - loss: 0.4786 - mean_squared_error: 0.4786 - acc: 0.4587\n",
      "Epoch 10/10\n",
      "303/303 [==============================] - 0s 40us/sample - loss: 0.4705 - mean_squared_error: 0.4705 - acc: 0.4653\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x172b36393c8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Base Model\n",
    "model = Sequential()\n",
    "model.add(Dense(15,input_dim=13, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(5, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "#Compile model\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mse', 'accuracy'])\n",
    "#fit model\n",
    "model.fit(X,y, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\test.NIDHNEMI\\Anaconda3\\envs\\U4-S2-NN\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.7095709443092346 using {'batch_size': 10, 'epochs': 100}\n",
      "Means: 0.49504952629407245, Stdev: 0.01400212435493574 with: {'batch_size': 10, 'epochs': 20}\n",
      "Means: 0.5577557782332102, Stdev: 0.0901418982927951 with: {'batch_size': 10, 'epochs': 50}\n",
      "Means: 0.7095709443092346, Stdev: 0.09334743013780726 with: {'batch_size': 10, 'epochs': 100}\n",
      "Means: 0.5478547910849253, Stdev: 0.0648411903608231 with: {'batch_size': 20, 'epochs': 20}\n",
      "Means: 0.5511551300684611, Stdev: 0.032671590713967885 with: {'batch_size': 20, 'epochs': 50}\n",
      "Means: 0.6897689700126648, Stdev: 0.09369682889158953 with: {'batch_size': 20, 'epochs': 100}\n",
      "Means: 0.43894390265146893, Stdev: 0.0382040845325417 with: {'batch_size': 40, 'epochs': 20}\n",
      "Means: 0.5445544620354971, Stdev: 0.06103379510077917 with: {'batch_size': 40, 'epochs': 50}\n",
      "Means: 0.5016501744588217, Stdev: 0.05259861470447439 with: {'batch_size': 40, 'epochs': 100}\n",
      "Means: 0.43564356366793316, Stdev: 0.029147730442540223 with: {'batch_size': 60, 'epochs': 20}\n",
      "Means: 0.4884488582611084, Stdev: 0.0459682915455473 with: {'batch_size': 60, 'epochs': 50}\n",
      "Means: 0.554455449183782, Stdev: 0.0450105003472179 with: {'batch_size': 60, 'epochs': 100}\n",
      "Means: 0.561056117216746, Stdev: 0.0373389608159633 with: {'batch_size': 80, 'epochs': 20}\n",
      "Means: 0.39603960514068604, Stdev: 0.0610338079929789 with: {'batch_size': 80, 'epochs': 50}\n",
      "Means: 0.46204620599746704, Stdev: 0.0283905166744344 with: {'batch_size': 80, 'epochs': 100}\n",
      "Means: 0.6072607239087423, Stdev: 0.009334740203990824 with: {'batch_size': 100, 'epochs': 20}\n",
      "Means: 0.49504949649175006, Stdev: 0.13454663527605618 with: {'batch_size': 100, 'epochs': 50}\n",
      "Means: 0.5214521586894989, Stdev: 0.0336568980862956 with: {'batch_size': 100, 'epochs': 100}\n"
     ]
    }
   ],
   "source": [
    "#Tune Base Model on batch_size and epochs\n",
    "\n",
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(15,input_dim=13, activation='relu'))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(5, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    # Compile model\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "# batch_size = [10, 20, 40, 60, 80, 100]\n",
    "# param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "# define the grid search parameters\n",
    "param_grid = {'batch_size': [10, 20, 40, 60, 80, 100],\n",
    "              'epochs': [20,50,100]}\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(X, y)\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0124 09:16:43.959601 16816 deprecation.py:506] From C:\\Users\\test.NIDHNEMI\\Anaconda3\\envs\\U4-S2-NN\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\adagrad.py:105: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.7293729384740194 using {'optimizer': 'RMSprop'}\n",
      "Means: 0.5445544719696045, Stdev: 0.021388576788767738 with: {'optimizer': 'SGD'}\n",
      "Means: 0.7293729384740194, Stdev: 0.12348703221579967 with: {'optimizer': 'RMSprop'}\n",
      "Means: 0.5247524778048197, Stdev: 0.11404059928368775 with: {'optimizer': 'Adagrad'}\n",
      "Means: 0.478547861178716, Stdev: 0.04452389737819455 with: {'optimizer': 'Adadelta'}\n",
      "Means: 0.6336633662382761, Stdev: 0.12072171961352073 with: {'optimizer': 'Adam'}\n",
      "Means: 0.6567656596501669, Stdev: 0.03820407080172275 with: {'optimizer': 'Adamax'}\n",
      "Means: 0.6864686409632365, Stdev: 0.0246974223731781 with: {'optimizer': 'Nadam'}\n"
     ]
    }
   ],
   "source": [
    "#Tune Base Model on suggested batch_size and epochs, on optimizer\n",
    "\n",
    "def create_model(optimizer = 'adam'):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(15,input_dim=13, activation='relu'))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(5, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    # Compile model\n",
    "    model.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model,  epochs=100, batch_size=10, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "param_grid = dict(optimizer=optimizer)\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(X, y)\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0124 09:29:25.016165 16816 deprecation.py:323] From C:\\Users\\test.NIDHNEMI\\Anaconda3\\envs\\U4-S2-NN\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.759075919787089 using {'activation': 'softsign'}\n",
      "Means: 0.5841584205627441, Stdev: 0.042777153577535476 with: {'activation': 'softmax'}\n",
      "Means: 0.5577557881673177, Stdev: 0.06779746687125664 with: {'activation': 'softplus'}\n",
      "Means: 0.759075919787089, Stdev: 0.009334740203990824 with: {'activation': 'softsign'}\n",
      "Means: 0.6567656993865967, Stdev: 0.10042985989191526 with: {'activation': 'relu'}\n",
      "Means: 0.6501650015513102, Stdev: 0.01866948040798165 with: {'activation': 'tanh'}\n",
      "Means: 0.6765676339467367, Stdev: 0.004667370101995413 with: {'activation': 'sigmoid'}\n",
      "Means: 0.636963685353597, Stdev: 0.02333685050997706 with: {'activation': 'hard_sigmoid'}\n",
      "Means: 0.5874587496121725, Stdev: 0.06279304264079835 with: {'activation': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "#Tune Base Model on suggested batch_size, epochs, optimizer on activation function\n",
    "\n",
    "def create_model(activation = 'relu'):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(15,input_dim=13, activation=activation))\n",
    "    model.add(Dense(10, activation=activation))\n",
    "    model.add(Dense(5, activation=activation))\n",
    "    model.add(Dense(1))\n",
    "    # Compile model\n",
    "    model.compile(loss='mse', optimizer='RMSprop', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model,  epochs=100, batch_size=10, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "param_grid = dict(activation=activation)\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(X, y)\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0124 09:38:17.667977 16816 nn_ops.py:4224] Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0124 09:38:33.692927 16816 nn_ops.py:4224] Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0124 09:38:49.352174 16816 nn_ops.py:4224] Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0124 09:39:05.225270 16816 nn_ops.py:4224] Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0124 09:39:22.716305 16816 nn_ops.py:4224] Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.7623762488365173 using {'dropout_rate': 0.3}\n",
      "Means: 0.7194719711939493, Stdev: 0.004667370101995412 with: {'dropout_rate': 0.0}\n",
      "Means: 0.7161716222763062, Stdev: 0.03060602036578158 with: {'dropout_rate': 0.1}\n",
      "Means: 0.6864686409632365, Stdev: 0.016828461706711845 with: {'dropout_rate': 0.2}\n",
      "Means: 0.7623762488365173, Stdev: 0.04042061077191993 with: {'dropout_rate': 0.3}\n",
      "Means: 0.6897689700126648, Stdev: 0.009334768301889833 with: {'dropout_rate': 0.4}\n",
      "Means: 0.6765676538149515, Stdev: 0.02839052937741819 with: {'dropout_rate': 0.5}\n",
      "Means: 0.669966995716095, Stdev: 0.03060602036578158 with: {'dropout_rate': 0.6}\n",
      "Means: 0.6402640144030253, Stdev: 0.03267159071396789 with: {'dropout_rate': 0.7}\n",
      "Means: 0.5445544719696045, Stdev: 0.021388576788767738 with: {'dropout_rate': 0.8}\n",
      "Means: 0.5445544719696045, Stdev: 0.021388576788767738 with: {'dropout_rate': 0.9}\n"
     ]
    }
   ],
   "source": [
    "#Tune Base Model on suggested batch_size, epochs, optimizer, activation function on dropout rate\n",
    "\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "def create_model(dropout_rate = '0.0'):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(15,input_dim=13, activation='softsign'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(10, activation='softsign'))\n",
    "    model.add(Dense(5, activation='softsign'))\n",
    "    model.add(Dense(1))\n",
    "    # Compile model\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model,  epochs=100, batch_size=10, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "param_grid = dict(dropout_rate=dropout_rate)\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(X, y)\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "U4-S2-NN",
   "language": "python",
   "name": "u4-s2-nn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
