{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Ryp-TVm4njD"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "\n",
    "# Hyperparameter Tuning\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignment 4*\n",
    "\n",
    "## Your Mission, should you choose to accept it...\n",
    "\n",
    "To hyperparameter tune and extract every ounce of accuracy out of this telecom customer churn dataset: <https://drive.google.com/file/d/1dfbAsM9DwA7tYhInyflIpZnYs7VT-0AQ/view> \n",
    "\n",
    "## Requirements\n",
    "\n",
    "- Load the data\n",
    "- Clean the data if necessary (it will be)\n",
    "- Create and fit a baseline Keras MLP model to the data.\n",
    "- Hyperparameter tune (at least) the following parameters:\n",
    " - batch_size\n",
    " - training epochs\n",
    " - optimizer\n",
    " - learning rate (if applicable to optimizer)\n",
    " - momentum (if applicable to optimizer)\n",
    " - activation functions\n",
    " - network weight initialization\n",
    " - dropout regularization\n",
    " - number of neurons in the hidden layer\n",
    " \n",
    " You must use Grid Search and Cross Validation for your initial pass of the above hyperparameters\n",
    " \n",
    " Try and get the maximum accuracy possible out of this data! You'll save big telecoms millions! Doesn't that sound great?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NNJ-tOBs4jM1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "dataframe = pandas.read_csv(\"WA_Fn-UseC_-Telco-Customer-Churn.csv\", delimiter=',', na_values=[\" \"])\n",
    "dataframe = dataframe.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows     :  7032\n",
      "Columns  :  21\n",
      "\n",
      "Features : \n",
      " ['customerID', 'gender', 'SeniorCitizen', 'Partner', 'Dependents', 'tenure', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod', 'MonthlyCharges', 'TotalCharges', 'Churn']\n",
      "\n",
      "Missing values :   0\n",
      "\n",
      "Unique values :  \n",
      " customerID          7032\n",
      "gender                 2\n",
      "SeniorCitizen          2\n",
      "Partner                2\n",
      "Dependents             2\n",
      "tenure                72\n",
      "PhoneService           2\n",
      "MultipleLines          3\n",
      "InternetService        3\n",
      "OnlineSecurity         3\n",
      "OnlineBackup           3\n",
      "DeviceProtection       3\n",
      "TechSupport            3\n",
      "StreamingTV            3\n",
      "StreamingMovies        3\n",
      "Contract               3\n",
      "PaperlessBilling       2\n",
      "PaymentMethod          4\n",
      "MonthlyCharges      1584\n",
      "TotalCharges        6530\n",
      "Churn                  2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print (\"Rows     : \" ,dataframe.shape[0])\n",
    "print (\"Columns  : \" ,dataframe.shape[1])\n",
    "print (\"\\nFeatures : \\n\" ,dataframe.columns.tolist())\n",
    "print (\"\\nMissing values :  \", dataframe.isnull().sum().values.sum())\n",
    "print (\"\\nUnique values :  \\n\",dataframe.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Replacing spaces with null values in total charges column\n",
    "dataframe['TotalCharges'] = dataframe[\"TotalCharges\"].replace(\" \",np.nan)\n",
    "\n",
    "#Dropping null values from total charges column which contain .15% missing data \n",
    "dataframe = dataframe[dataframe[\"TotalCharges\"].notnull()]\n",
    "dataframe = dataframe.reset_index()[dataframe.columns]\n",
    "\n",
    "#convert to float type\n",
    "dataframe[\"TotalCharges\"] = dataframe[\"TotalCharges\"].astype(float)\n",
    "\n",
    "#replace 'No internet service' to No for the following columns\n",
    "replace_cols = [ 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection',\n",
    "                'TechSupport','StreamingTV', 'StreamingMovies']\n",
    "for i in replace_cols : \n",
    "    dataframe[i]  = dataframe[i].replace({'No internet service' : 'No'})\n",
    "    \n",
    "#replace values\n",
    "dataframe[\"SeniorCitizen\"] = dataframe[\"SeniorCitizen\"].replace({1:\"Yes\",0:\"No\"})\n",
    "\n",
    "#Tenure to categorical column\n",
    "def tenure_lab(dataframe) :\n",
    "    \n",
    "    if dataframe[\"tenure\"] <= 12 :\n",
    "        return \"Tenure_0-12\"\n",
    "    elif (dataframe[\"tenure\"] > 12) & (dataframe[\"tenure\"] <= 24 ):\n",
    "        return \"Tenure_12-24\"\n",
    "    elif (dataframe[\"tenure\"] > 24) & (dataframe[\"tenure\"] <= 48) :\n",
    "        return \"Tenure_24-48\"\n",
    "    elif (dataframe[\"tenure\"] > 48) & (dataframe[\"tenure\"] <= 60) :\n",
    "        return \"Tenure_48-60\"\n",
    "    elif dataframe[\"tenure\"] > 60 :\n",
    "        return \"Tenure_gt_60\"\n",
    "dataframe[\"tenure_group\"] = dataframe.apply(lambda dataframe:tenure_lab(dataframe),\n",
    "                                      axis = 1)\n",
    "\n",
    "#Separating churn and non churn customers\n",
    "churn     = dataframe[dataframe[\"Churn\"] == \"Yes\"]\n",
    "not_churn = dataframe[dataframe[\"Churn\"] == \"No\"]\n",
    "\n",
    "#Separating catagorical and numerical columns\n",
    "Id_col     = ['customerID']\n",
    "target_col = [\"Churn\"]\n",
    "cat_cols   = dataframe.nunique()[dataframe.nunique() < 6].keys().tolist()\n",
    "cat_cols   = [x for x in cat_cols if x not in target_col]\n",
    "num_cols   = [x for x in dataframe.columns if x not in cat_cols + target_col + Id_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "#customer id col\n",
    "Id_col     = ['customerID']\n",
    "#Target columns\n",
    "target_col = [\"Churn\"]\n",
    "#categorical columns\n",
    "cat_cols   = dataframe.nunique()[dataframe.nunique() < 6].keys().tolist()\n",
    "cat_cols   = [x for x in cat_cols if x not in target_col]\n",
    "#numerical columns\n",
    "num_cols   = [x for x in dataframe.columns if x not in cat_cols + target_col + Id_col]\n",
    "#Binary columns with 2 values\n",
    "bin_cols   = dataframe.nunique()[dataframe.nunique() == 2].keys().tolist()\n",
    "#Columns more than 2 values\n",
    "multi_cols = [i for i in cat_cols if i not in bin_cols]\n",
    "\n",
    "#Label encoding Binary columns\n",
    "le = LabelEncoder()\n",
    "for i in bin_cols :\n",
    "    dataframe[i] = le.fit_transform(dataframe[i])\n",
    "    \n",
    "#Duplicating columns for multi value columns\n",
    "dataframe = pd.get_dummies(data = dataframe,columns = multi_cols )\n",
    "\n",
    "#Scaling Numerical columns\n",
    "std = StandardScaler()\n",
    "scaled = std.fit_transform(dataframe[num_cols])\n",
    "scaled = pd.DataFrame(scaled,columns=num_cols)\n",
    "\n",
    "#dropping original values merging scaled values for numerical columns\n",
    "df_telcom_og = dataframe.copy()\n",
    "dataframe = dataframe.drop(columns = num_cols,axis = 1)\n",
    "dataframe = dataframe.merge(scaled,left_index=True,right_index=True,how = \"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gender                                       int64\n",
       "SeniorCitizen                                int64\n",
       "Partner                                      int64\n",
       "Dependents                                   int64\n",
       "PhoneService                                 int64\n",
       "OnlineSecurity                               int64\n",
       "OnlineBackup                                 int64\n",
       "DeviceProtection                             int64\n",
       "TechSupport                                  int64\n",
       "StreamingTV                                  int64\n",
       "StreamingMovies                              int64\n",
       "PaperlessBilling                             int64\n",
       "Churn                                        int64\n",
       "MultipleLines_No                             uint8\n",
       "MultipleLines_No phone service               uint8\n",
       "MultipleLines_Yes                            uint8\n",
       "InternetService_DSL                          uint8\n",
       "InternetService_Fiber optic                  uint8\n",
       "InternetService_No                           uint8\n",
       "Contract_Month-to-month                      uint8\n",
       "Contract_One year                            uint8\n",
       "Contract_Two year                            uint8\n",
       "PaymentMethod_Bank transfer (automatic)      uint8\n",
       "PaymentMethod_Credit card (automatic)        uint8\n",
       "PaymentMethod_Electronic check               uint8\n",
       "PaymentMethod_Mailed check                   uint8\n",
       "tenure_group_Tenure_0-12                     uint8\n",
       "tenure_group_Tenure_12-24                    uint8\n",
       "tenure_group_Tenure_24-48                    uint8\n",
       "tenure_group_Tenure_48-60                    uint8\n",
       "tenure_group_Tenure_gt_60                    uint8\n",
       "tenure                                     float64\n",
       "MonthlyCharges                             float64\n",
       "TotalCharges                               float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.drop(\"customerID\", axis= 1, inplace= True)\n",
    "dataset = dataframe.values\n",
    "dataframe.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into input (X) and output (Y) variables\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = dataset[:,0:20]\n",
    "y = dataset[:,20]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4711 samples, validate on 2321 samples\n",
      "Epoch 1/5\n",
      "4711/4711 [==============================] - 1s 192us/step - loss: 0.1357 - mse: 0.1357 - binary_accuracy: 0.7939 - val_loss: 0.1014 - val_mse: 0.1014 - val_binary_accuracy: 0.8341\n",
      "Epoch 2/5\n",
      "4711/4711 [==============================] - 1s 144us/step - loss: 0.0982 - mse: 0.0982 - binary_accuracy: 0.8431 - val_loss: 0.0912 - val_mse: 0.0912 - val_binary_accuracy: 0.8630\n",
      "Epoch 3/5\n",
      "4711/4711 [==============================] - 1s 143us/step - loss: 0.0943 - mse: 0.0943 - binary_accuracy: 0.8459 - val_loss: 0.0959 - val_mse: 0.0959 - val_binary_accuracy: 0.8514\n",
      "Epoch 4/5\n",
      "4711/4711 [==============================] - 1s 166us/step - loss: 0.0934 - mse: 0.0934 - binary_accuracy: 0.8527 - val_loss: 0.0906 - val_mse: 0.0906 - val_binary_accuracy: 0.8587\n",
      "Epoch 5/5\n",
      "4711/4711 [==============================] - 1s 157us/step - loss: 0.0918 - mse: 0.0918 - binary_accuracy: 0.8544 - val_loss: 0.0915 - val_mse: 0.0915 - val_binary_accuracy: 0.8591\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x18d668bc108>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Important Hyperparameters\n",
    "inputs = X_train.shape[1]\n",
    "epochs = 5\n",
    "batch_size = 10\n",
    "\n",
    "\n",
    "# Create Model\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_shape=(inputs,)))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile Model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mse', 'binary_accuracy'])\n",
    "\n",
    "# Fit Model\n",
    "model.fit(X_train, y_train, \n",
    "          validation_data=(X_test,y_test), \n",
    "          epochs=epochs, \n",
    "          batch_size=batch_size\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.8526734908421835 using {'batch_size': 40, 'epochs': 20}\n",
      "Means: 0.8408703009287516, Stdev: 0.009753379008365527 with: {'batch_size': 10, 'epochs': 20}\n",
      "Means: 0.8478384613990784, Stdev: 0.006538429044190065 with: {'batch_size': 20, 'epochs': 20}\n",
      "Means: 0.8526734908421835, Stdev: 0.007869070447583835 with: {'batch_size': 40, 'epochs': 20}\n",
      "Means: 0.8494027455647787, Stdev: 0.002118840829979028 with: {'batch_size': 60, 'epochs': 20}\n",
      "Means: 0.8516780336697897, Stdev: 0.0068288958474473395 with: {'batch_size': 80, 'epochs': 20}\n",
      "Means: 0.8515358368555704, Stdev: 0.008750056300480384 with: {'batch_size': 100, 'epochs': 20}\n"
     ]
    }
   ],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation='relu', input_shape=(inputs,)))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mse', 'accuracy'])\n",
    "    return model\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "# batch_size = [10, 20, 40, 60, 80, 100]\n",
    "# param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "# define the grid search parameters\n",
    "param_grid = {'batch_size': [10, 20, 40, 60, 80, 100],\n",
    "              'epochs': [20]}\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
    "grid_result = grid.fit(X, y)\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FfZRtJ7MCN3x"
   },
   "source": [
    "## Stretch Goals:\n",
    "\n",
    "- Try to implement Random Search Hyperparameter Tuning on this dataset\n",
    "- Try to implement Bayesian Optimiation tuning on this dataset using hyperas or hyperopt (if you're brave)\n",
    "- Practice hyperparameter tuning other datasets that we have looked at. How high can you get MNIST? Above 99%?\n",
    "- Study for the Sprint Challenge\n",
    " - Can you implement both perceptron and MLP models from scratch with forward and backpropagation?\n",
    " - Can you implement both perceptron and MLP models in keras and tune their hyperparameters with cross validation?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "LS_DS_434_Hyperparameter_Tuning_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "U4-S2-NN",
   "language": "python",
   "name": "u4-s2-nn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
